{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3582408",
   "metadata": {},
   "source": [
    "# BPE DA ADDESTRARE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0da48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8290e4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPEPipeline:\n",
    "    def __init__(self, vocab_size):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tokenizer = None\n",
    "        self.bpe = None\n",
    "\n",
    "    def fit(self, corpus):\n",
    "        self.bpe = BPE(corpus, self.vocab_size)\n",
    "        self.bpe.train()\n",
    "    \n",
    "    def transform(self, text):\n",
    "        if self.bpe is None:\n",
    "            raise RuntimeError(\"Il tokenizzatore BPE non è stato ancora addestrato. Chiama prima il metodo fit.\")\n",
    "        return self.bpe.tokenize(text)\n",
    "\n",
    "# Classe BPE \n",
    "class BPE:\n",
    "    \n",
    "    \n",
    "    def __init__(self, corpus, vocab_size):\n",
    "        \"\"\"Initialize BPE tokenizer.\"\"\"\n",
    "        self.corpus = corpus\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # pre-tokenize the corpus into words, BERT pre-tokenizer is used here\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "        self.word_freqs = defaultdict(int)\n",
    "        self.splits = {}\n",
    "        self.merges = {}\n",
    "    \n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Train BPE tokenizer.\"\"\"\n",
    "\n",
    "        # compute the frequencies of each word in the corpus\n",
    "        for text in self.corpus:\n",
    "            words_with_offsets = self.tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "            new_words = [word for word, offset in words_with_offsets]\n",
    "            for word in new_words:\n",
    "                self.word_freqs[word] += 1\n",
    "\n",
    "        # compute the base vocabulary of all characters in the corpus\n",
    "        alphabet = []\n",
    "        for word in self.word_freqs.keys():\n",
    "            for letter in word:\n",
    "                if letter not in alphabet:\n",
    "                    alphabet.append(letter)\n",
    "        alphabet.sort()\n",
    "\n",
    "        # add the special token </w> at the beginning of the vocabulary\n",
    "        vocab = [\"</w>\"] + alphabet.copy()\n",
    "\n",
    "        # split each word into individual characters before training\n",
    "        self.splits = {word: [c for c in word] for word in self.word_freqs.keys()}\n",
    "\n",
    "        # merge the most frequent pair iteratively until the vocabulary size is reached\n",
    "        while len(vocab) < self.vocab_size:\n",
    "\n",
    "            # compute the frequency of each pair\n",
    "            pair_freqs = self.compute_pair_freqs()\n",
    "\n",
    "            # find the most frequent pair\n",
    "            best_pair = \"\"\n",
    "            max_freq = None\n",
    "            for pair, freq in pair_freqs.items():\n",
    "                if max_freq is None or max_freq < freq:\n",
    "                    best_pair = pair\n",
    "                    max_freq = freq\n",
    "\n",
    "            # merge the most frequent pair\n",
    "            self.splits = self.merge_pair(*best_pair)\n",
    "            self.merges[best_pair] = best_pair[0] + best_pair[1]\n",
    "            vocab.append(best_pair[0] + best_pair[1])\n",
    "        return self.merges\n",
    "\n",
    "\n",
    "    def compute_pair_freqs(self):\n",
    "        \"\"\"Compute the frequency of each pair.\"\"\"\n",
    "\n",
    "        pair_freqs = defaultdict(int)\n",
    "        for word, freq in self.word_freqs.items():\n",
    "            split = self.splits[word]\n",
    "            if len(split) == 1:\n",
    "                continue\n",
    "            for i in range(len(split) - 1):\n",
    "                pair = (split[i], split[i + 1])\n",
    "                pair_freqs[pair] += freq\n",
    "        return pair_freqs\n",
    "\n",
    "\n",
    "    def merge_pair(self, a, b):\n",
    "        \"\"\"Merge the given pair.\"\"\"\n",
    "\n",
    "        for word in self.word_freqs:\n",
    "            split = self.splits[word]\n",
    "            if len(split) == 1:\n",
    "                continue\n",
    "            i = 0\n",
    "            while i < len(split) - 1:\n",
    "                if split[i] == a and split[i + 1] == b:\n",
    "                    split = split[:i] + [a + b] + split[i + 2 :]\n",
    "                else:\n",
    "                    i += 1\n",
    "            self.splits[word] = split\n",
    "        return self.splits\n",
    "    \n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Tokenize a given text with trained BPE tokenizer (including pre-tokenization, split, and merge).\"\"\"\n",
    "        \n",
    "        pre_tokenize_result = self.tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "        pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n",
    "        splits_text = [[l for l in word] for word in pre_tokenized_text]\n",
    "\n",
    "        for pair, merge in self.merges.items():\n",
    "            for idx, split in enumerate(splits_text):\n",
    "                i = 0\n",
    "                while i < len(split) - 1:\n",
    "                    if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
    "                        split = split[:i] + [merge] + split[i + 2 :]\n",
    "                    else:\n",
    "                        i += 1\n",
    "                splits_text[idx] = split\n",
    "        result = sum(splits_text, [])\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf2cb9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importare i datset\n",
    "train_set = pd.read_csv('train.csv')\n",
    "test_set = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952fdcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizzo della pipeline BPE\n",
    "# Sostituire con il  corpus sul quale si desidera addestrare bpe\n",
    "corpus = train_set['Title']\n",
    "\n",
    "vocab_size = 1000\n",
    "\n",
    "bpe_pipeline = BPEPipeline(vocab_size)\n",
    "bpe_pipeline.fit(corpus)\n",
    "\n",
    "\n",
    "# Esempio di tokenizzazione\n",
    "text = \"Esempio di testo da tokenizzare\"\n",
    "tokenized_text = bpe_pipeline.transform(text)\n",
    "print(tokenized_text)\n",
    "\n",
    "\n",
    "# Esempio di tokenizzazione BPE al dataset\n",
    "def apply_bpe_to_series(series):\n",
    "    return series.apply(bpe_pipeline.transform)\n",
    "\n",
    "# Applicare la funzione alla colonna desiderata del DataFrame\n",
    "train_set['Tokenized'] = apply_bpe_to_series(train_set['Description'])\n",
    "train_set['Tokenized'].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4314b0e",
   "metadata": {},
   "source": [
    "# BPE PREADDESTRATO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f039bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43e39451",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "class BpeRobertaPipeline:\n",
    "    def __init__(self, model_name='roberta-base'):\n",
    "        self.tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    def process_text(self, text):\n",
    "        # Tokenizza il testo\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "        # Converte i token in una stringa\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    def apply_pipeline(self, df, column_name):\n",
    "        # Applica la pipeline di processamento del testo alla colonna specificata\n",
    "        return df[column_name].apply(self.process_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcce06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esempio di Utilizzo BpeRobertaPipeline\n",
    "\n",
    "pipeline = BpeRobertaPipeline()\n",
    "train_set['Processed_Text'] = pipeline.apply_pipeline(train_set, 'Description')\n",
    "\n",
    "print(train_set['Processed_Text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b32c29",
   "metadata": {},
   "source": [
    "# PRE-PROCESSING INIZIALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b48973e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/sara/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class Index</th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>Tokenized</th>\n",
       "      <th>Text</th>\n",
       "      <th>Processed_Text</th>\n",
       "      <th>clear_text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reuters)</td>\n",
       "      <td>Reuters - Short-sellers, Wall Street's dwindli...</td>\n",
       "      <td>[Reuters, Ġ-, ĠSh, ort, -, s, ell, ers, ,, ĠW,...</td>\n",
       "      <td>Reuters Ġ- ĠShort - sell ers , ĠWall ĠStreet '...</td>\n",
       "      <td>Reuters Ġ- ĠShort - sell ers , ĠWall ĠStreet '...</td>\n",
       "      <td>reuters short sellers wall street dwindling ba...</td>\n",
       "      <td>reuters short sellers wall street dwindling ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "      <td>Reuters - Private investment firm Carlyle Grou...</td>\n",
       "      <td>[Reuters, Ġ-, ĠP, riv, ate, Ġin, vest, ment, Ġ...</td>\n",
       "      <td>Reuters Ġ- ĠPrivate Ġinvestment Ġfirm ĠCarly l...</td>\n",
       "      <td>Reuters Ġ- ĠPrivate Ġinvestment Ġfirm ĠCarly l...</td>\n",
       "      <td>reuters private investment firm carlyle group ...</td>\n",
       "      <td>reuters private investment firm carlyle group ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters)</td>\n",
       "      <td>Reuters - Soaring crude prices plus worries\\ab...</td>\n",
       "      <td>[Reuters, Ġ-, ĠS, o, ar, ing, Ġcr, ud, e, Ġpri...</td>\n",
       "      <td>Reuters Ġ- ĠSo aring Ġcrude Ġprices Ġplus Ġwor...</td>\n",
       "      <td>Reuters Ġ- ĠSo aring Ġcrude Ġprices Ġplus Ġwor...</td>\n",
       "      <td>reuters soaring crude prices plus worries econ...</td>\n",
       "      <td>reuters soaring crude prices plus worries econ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "      <td>Reuters - Authorities have halted oil export\\f...</td>\n",
       "      <td>[Reuters, Ġ-, ĠA, ut, h, or, it, ies, Ġh, ave,...</td>\n",
       "      <td>Reuters Ġ- ĠAuthorities Ġhave Ġhalted Ġoil Ġex...</td>\n",
       "      <td>Reuters Ġ- ĠAuthorities Ġhave Ġhalted Ġoil Ġex...</td>\n",
       "      <td>reuters authorities halted oil export flows ma...</td>\n",
       "      <td>reuters authorities halted oil export flows ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "      <td>AFP - Tearaway world oil prices, toppling reco...</td>\n",
       "      <td>[AFP, Ġ-, ĠT, ear, aw, ay, Ġw, orld, Ġoil, Ġpr...</td>\n",
       "      <td>AFP Ġ- ĠT ear away Ġworld Ġoil Ġprices , Ġtopp...</td>\n",
       "      <td>AFP Ġ- ĠT ear away Ġworld Ġoil Ġprices , Ġtopp...</td>\n",
       "      <td>afp tearaway world oil prices toppling records...</td>\n",
       "      <td>afp tearaway world oil prices toppling records...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class Index                                              Title  \\\n",
       "0            3  Wall St. Bears Claw Back Into the Black (Reuters)   \n",
       "1            3  Carlyle Looks Toward Commercial Aerospace (Reu...   \n",
       "2            3    Oil and Economy Cloud Stocks' Outlook (Reuters)   \n",
       "3            3  Iraq Halts Oil Exports from Main Southern Pipe...   \n",
       "4            3  Oil prices soar to all-time record, posing new...   \n",
       "\n",
       "                                         Description  \\\n",
       "0  Reuters - Short-sellers, Wall Street's dwindli...   \n",
       "1  Reuters - Private investment firm Carlyle Grou...   \n",
       "2  Reuters - Soaring crude prices plus worries\\ab...   \n",
       "3  Reuters - Authorities have halted oil export\\f...   \n",
       "4  AFP - Tearaway world oil prices, toppling reco...   \n",
       "\n",
       "                                           Tokenized  \\\n",
       "0  [Reuters, Ġ-, ĠSh, ort, -, s, ell, ers, ,, ĠW,...   \n",
       "1  [Reuters, Ġ-, ĠP, riv, ate, Ġin, vest, ment, Ġ...   \n",
       "2  [Reuters, Ġ-, ĠS, o, ar, ing, Ġcr, ud, e, Ġpri...   \n",
       "3  [Reuters, Ġ-, ĠA, ut, h, or, it, ies, Ġh, ave,...   \n",
       "4  [AFP, Ġ-, ĠT, ear, aw, ay, Ġw, orld, Ġoil, Ġpr...   \n",
       "\n",
       "                                                Text  \\\n",
       "0  Reuters Ġ- ĠShort - sell ers , ĠWall ĠStreet '...   \n",
       "1  Reuters Ġ- ĠPrivate Ġinvestment Ġfirm ĠCarly l...   \n",
       "2  Reuters Ġ- ĠSo aring Ġcrude Ġprices Ġplus Ġwor...   \n",
       "3  Reuters Ġ- ĠAuthorities Ġhave Ġhalted Ġoil Ġex...   \n",
       "4  AFP Ġ- ĠT ear away Ġworld Ġoil Ġprices , Ġtopp...   \n",
       "\n",
       "                                      Processed_Text  \\\n",
       "0  Reuters Ġ- ĠShort - sell ers , ĠWall ĠStreet '...   \n",
       "1  Reuters Ġ- ĠPrivate Ġinvestment Ġfirm ĠCarly l...   \n",
       "2  Reuters Ġ- ĠSo aring Ġcrude Ġprices Ġplus Ġwor...   \n",
       "3  Reuters Ġ- ĠAuthorities Ġhave Ġhalted Ġoil Ġex...   \n",
       "4  AFP Ġ- ĠT ear away Ġworld Ġoil Ġprices , Ġtopp...   \n",
       "\n",
       "                                          clear_text  \\\n",
       "0  reuters short sellers wall street dwindling ba...   \n",
       "1  reuters private investment firm carlyle group ...   \n",
       "2  reuters soaring crude prices plus worries econ...   \n",
       "3  reuters authorities halted oil export flows ma...   \n",
       "4  afp tearaway world oil prices toppling records...   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  reuters short sellers wall street dwindling ba...  \n",
       "1  reuters private investment firm carlyle group ...  \n",
       "2  reuters soaring crude prices plus worries econ...  \n",
       "3  reuters authorities halted oil export flows ma...  \n",
       "4  afp tearaway world oil prices toppling records...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "# Scarica le stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def clean_text(text_column):\n",
    "    # Definisci le stopwords in inglese come un set\n",
    "    stop = set(stopwords.words('english'))\n",
    "\n",
    "    # Compila l'espressione regolare per la pulizia del testo\n",
    "    reg_exp = re.compile(r'[^a-zA-Z]+')\n",
    "\n",
    "    # Funzione per pulire una singola stringa\n",
    "    def clean_string(text):\n",
    "        text = text.lower()  # Converti in minuscolo\n",
    "        text = reg_exp.sub(' ', text)  # Sostituisci i caratteri non alfabetici con spazi\n",
    "        return ' '.join(w for w in text.split() if len(w) > 1 and w not in stop)\n",
    "\n",
    "    return clean_string\n",
    "\n",
    "# Esempio di utilizzo\n",
    "# Assicurati che 'train_set' sia il tuo DataFrame e 'Description' la colonna di testo\n",
    "# train_set = pd.DataFrame(...) # Il tuo DataFrame va qui\n",
    "\n",
    "clean_func = clean_text('Description')\n",
    "train_set['cleaned_text'] = train_set['Description'].apply(clean_func)\n",
    "train_set.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d909165f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
