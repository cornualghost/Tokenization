{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ab6f509",
   "metadata": {},
   "source": [
    "# BPE DA ADDESTRARE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce06fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0941f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPEPipeline:\n",
    "    def __init__(self, vocab_size):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tokenizer = None\n",
    "        self.bpe = None\n",
    "\n",
    "    def fit(self, corpus):\n",
    "        self.bpe = BPE(corpus, self.vocab_size)\n",
    "        self.bpe.train()\n",
    "    \n",
    "    def transform(self, text):\n",
    "        if self.bpe is None:\n",
    "            raise RuntimeError(\"Il tokenizzatore BPE non Ã¨ stato ancora addestrato. Chiama prima il metodo fit.\")\n",
    "        return self.bpe.tokenize(text)\n",
    "\n",
    "# Classe BPE \n",
    "class BPE:\n",
    "    \n",
    "    \n",
    "    def __init__(self, corpus, vocab_size):\n",
    "        \"\"\"Initialize BPE tokenizer.\"\"\"\n",
    "        self.corpus = corpus\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # pre-tokenize the corpus into words, BERT pre-tokenizer is used here\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "        self.word_freqs = defaultdict(int)\n",
    "        self.splits = {}\n",
    "        self.merges = {}\n",
    "    \n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Train BPE tokenizer.\"\"\"\n",
    "\n",
    "        # compute the frequencies of each word in the corpus\n",
    "        for text in self.corpus:\n",
    "            words_with_offsets = self.tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "            new_words = [word for word, offset in words_with_offsets]\n",
    "            for word in new_words:\n",
    "                self.word_freqs[word] += 1\n",
    "\n",
    "        # compute the base vocabulary of all characters in the corpus\n",
    "        alphabet = []\n",
    "        for word in self.word_freqs.keys():\n",
    "            for letter in word:\n",
    "                if letter not in alphabet:\n",
    "                    alphabet.append(letter)\n",
    "        alphabet.sort()\n",
    "\n",
    "        # add the special token </w> at the beginning of the vocabulary\n",
    "        vocab = [\"</w>\"] + alphabet.copy()\n",
    "\n",
    "        # split each word into individual characters before training\n",
    "        self.splits = {word: [c for c in word] for word in self.word_freqs.keys()}\n",
    "\n",
    "        # merge the most frequent pair iteratively until the vocabulary size is reached\n",
    "        while len(vocab) < self.vocab_size:\n",
    "\n",
    "            # compute the frequency of each pair\n",
    "            pair_freqs = self.compute_pair_freqs()\n",
    "\n",
    "            # find the most frequent pair\n",
    "            best_pair = \"\"\n",
    "            max_freq = None\n",
    "            for pair, freq in pair_freqs.items():\n",
    "                if max_freq is None or max_freq < freq:\n",
    "                    best_pair = pair\n",
    "                    max_freq = freq\n",
    "\n",
    "            # merge the most frequent pair\n",
    "            self.splits = self.merge_pair(*best_pair)\n",
    "            self.merges[best_pair] = best_pair[0] + best_pair[1]\n",
    "            vocab.append(best_pair[0] + best_pair[1])\n",
    "        return self.merges\n",
    "\n",
    "\n",
    "    def compute_pair_freqs(self):\n",
    "        \"\"\"Compute the frequency of each pair.\"\"\"\n",
    "\n",
    "        pair_freqs = defaultdict(int)\n",
    "        for word, freq in self.word_freqs.items():\n",
    "            split = self.splits[word]\n",
    "            if len(split) == 1:\n",
    "                continue\n",
    "            for i in range(len(split) - 1):\n",
    "                pair = (split[i], split[i + 1])\n",
    "                pair_freqs[pair] += freq\n",
    "        return pair_freqs\n",
    "\n",
    "\n",
    "    def merge_pair(self, a, b):\n",
    "        \"\"\"Merge the given pair.\"\"\"\n",
    "\n",
    "        for word in self.word_freqs:\n",
    "            split = self.splits[word]\n",
    "            if len(split) == 1:\n",
    "                continue\n",
    "            i = 0\n",
    "            while i < len(split) - 1:\n",
    "                if split[i] == a and split[i + 1] == b:\n",
    "                    split = split[:i] + [a + b] + split[i + 2 :]\n",
    "                else:\n",
    "                    i += 1\n",
    "            self.splits[word] = split\n",
    "        return self.splits\n",
    "    \n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Tokenize a given text with trained BPE tokenizer (including pre-tokenization, split, and merge).\"\"\"\n",
    "        \n",
    "        pre_tokenize_result = self.tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "        pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n",
    "        splits_text = [[l for l in word] for word in pre_tokenized_text]\n",
    "\n",
    "        for pair, merge in self.merges.items():\n",
    "            for idx, split in enumerate(splits_text):\n",
    "                i = 0\n",
    "                while i < len(split) - 1:\n",
    "                    if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
    "                        split = split[:i] + [merge] + split[i + 2 :]\n",
    "                    else:\n",
    "                        i += 1\n",
    "                splits_text[idx] = split\n",
    "        result = sum(splits_text, [])\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "51b4a795",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importare i datset\n",
    "train_set = pd.read_csv('train.csv')\n",
    "test_set = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d815118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizzo della pipeline BPE\n",
    "# Sostituire con il  corpus sul quale si desidera addestrare bpe\n",
    "corpus = train_set['Title']\n",
    "\n",
    "vocab_size = 1000\n",
    "\n",
    "bpe_pipeline = BPEPipeline(vocab_size)\n",
    "bpe_pipeline.fit(corpus)\n",
    "\n",
    "\n",
    "# Esempio di tokenizzazione\n",
    "text = \"Esempio di testo da tokenizzare\"\n",
    "tokenized_text = bpe_pipeline.transform(text)\n",
    "print(tokenized_text)\n",
    "\n",
    "\n",
    "# Esempio di tokenizzazione BPE al dataset\n",
    "def apply_bpe_to_series(series):\n",
    "    return series.apply(bpe_pipeline.transform)\n",
    "\n",
    "# Applicare la funzione alla colonna desiderata del DataFrame\n",
    "train_set['Tokenized'] = apply_bpe_to_series(train_set['Description'])\n",
    "train_set['Tokenized'].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8242153b",
   "metadata": {},
   "source": [
    "# BPE PREADDESTRATO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c55ad34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4fe93344",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "class BpeRobertaPipeline:\n",
    "    def __init__(self, model_name='roberta-base'):\n",
    "        self.tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    def process_text(self, text):\n",
    "        # Tokenizza il testo\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "        # Converte i token in una stringa\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    def apply_pipeline(self, df, column_name):\n",
    "        # Applica la pipeline di processamento del testo alla colonna specificata\n",
    "        return df[column_name].apply(self.process_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e44ea4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esempio di Utilizzo BpeRobertaPipeline\n",
    "\n",
    "pipeline = BpeRobertaPipeline()\n",
    "train_set['Processed_Text'] = pipeline.apply_pipeline(train_set, 'Description')\n",
    "\n",
    "print(train_set['Processed_Text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
