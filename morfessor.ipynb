{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f10a9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import morfessor\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e319cdaf",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'int' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Creare un'istanza del modello Morfessor e caricare i dati\u001b[39;00m\n\u001b[1;32m     26\u001b[0m model \u001b[38;5;241m=\u001b[39m morfessor\u001b[38;5;241m.\u001b[39mBaselineModel()\n\u001b[0;32m---> 27\u001b[0m model\u001b[38;5;241m.\u001b[39mload_data(train_data, count_modifier\u001b[38;5;241m=\u001b[39mlog_func)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Addestrare il modello\u001b[39;00m\n\u001b[1;32m     30\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain_batch()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/morfessor/baseline.py:500\u001b[0m, in \u001b[0;36mBaselineModel.load_data\u001b[0;34m(self, data, freqthreshold, count_modifier, init_rand_split)\u001b[0m\n\u001b[1;32m    498\u001b[0m totalcount \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mCounter()\n\u001b[1;32m    499\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m count, atoms \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[0;32m--> 500\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(atoms) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    501\u001b[0m         totalcount[atoms] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m count\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m atoms, count \u001b[38;5;129;01min\u001b[39;00m totalcount\u001b[38;5;241m.\u001b[39mitems():\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'int' has no len()"
     ]
    }
   ],
   "source": [
    "# Caricare il dataset\n",
    "train_set = pd.read_csv('train.csv')\n",
    "\n",
    "\n",
    "# Funzione per aggiustare i conteggi di ogni composto\n",
    "def log_func(x):\n",
    "    return int(round(math.log(x + 1, 2)))\n",
    "\n",
    "# Funzione per aggiustare i conteggi di ogni composto\n",
    "def log_func(x):\n",
    "    return int(round(math.log(x + 1, 2)))\n",
    "\n",
    "# Creazione di un'istanza per I/O e del modello Morfessor\n",
    "io = morfessor.MorfessorIO()\n",
    "\n",
    "# Leggere i dati dal file di testo\n",
    "with open('titoli_per_morfessor.txt', 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Preparazione dei dati per Morfessor\n",
    "# Si assume che ogni riga del file sia un esempio di input separato\n",
    "train_data = [(line.strip(), log_func(1)) for line in lines if line.strip()]\n",
    "\n",
    "\n",
    "# Creare un'istanza del modello Morfessor e caricare i dati\n",
    "model = morfessor.BaselineModel()\n",
    "model.load_data(train_data, count_modifier=log_func)\n",
    "\n",
    "# Addestrare il modello\n",
    "model.train_batch()\n",
    "\n",
    "# Salvare il modello addestrato\n",
    "io.write_binary_model_file(\"model.bin\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5fae71d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caricare il dataset\n",
    "train_set = pd.read_csv('train.csv')\n",
    "\n",
    "# Estrazione solo dei titoli (o modifica questa linea per estrarre la parte desiderata)\n",
    "titoli = train_set['Title']\n",
    "\n",
    "# Salvare i titoli in un nuovo file di testo\n",
    "path_del_file_titoli = 'titoli_per_morfessor.txt'\n",
    "with open(path_del_file_titoli, 'w', encoding='utf-8') as file:\n",
    "    for titolo in titoli:\n",
    "        file.write(titolo + '\\n')\n",
    "\n",
    "# Ora puoi usare 'titoli_per_morfessor.txt' come infile per Morfessor\n",
    "infile = path_del_file_titoli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9fbd5d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per aggiustare i conteggi di ogni composto\n",
    "def log_func(x):\n",
    "    return int(round(math.log(x + 1, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bcd28b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "...........................................................\n",
      "...........................................................\n",
      "...........................................................\n",
      "...........................................................\n",
      "...........................................................\n"
     ]
    }
   ],
   "source": [
    "# Creazione di un'istanza per I/O e del modello Morfessor\n",
    "io = morfessor.MorfessorIO()\n",
    "train_data = list(io.read_corpus_file(infile))\n",
    "\n",
    "# Creare un'istanza del modello Morfessor e caricare i dati\n",
    "model = morfessor.BaselineModel()\n",
    "model.load_data(train_data, count_modifier=log_func)\n",
    "\n",
    "# Addestrare il modello\n",
    "model.train_batch()\n",
    "\n",
    "# Salvare il modello addestrato\n",
    "io.write_binary_model_file(\"model.bin\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b3d63d00",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m tokenized_titles \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m title \u001b[38;5;129;01min\u001b[39;00m titles:\n\u001b[0;32m---> 14\u001b[0m     segmentation \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mviterbi_segment(title)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     15\u001b[0m     tokenized_titles\u001b[38;5;241m.\u001b[39mappend(segmentation)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Aggiungere i titoli tokenizzati al dataframe\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/morfessor/baseline.py:775\u001b[0m, in \u001b[0;36mBaselineModel.viterbi_segment\u001b[0;34m(self, compound, addcount, maxlen)\u001b[0m\n\u001b[1;32m    770\u001b[0m         cost \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (addcount \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39mlog(addcount) \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    771\u001b[0m                  \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lexicon_coding\u001b[38;5;241m.\u001b[39mget_codelength(\n\u001b[1;32m    772\u001b[0m                      construction)\n\u001b[1;32m    773\u001b[0m                  \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_corpus_coding\u001b[38;5;241m.\u001b[39mweight)\n\u001b[1;32m    774\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 775\u001b[0m         cost \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (logtokens \u001b[38;5;241m-\u001b[39m math\u001b[38;5;241m.\u001b[39mlog(addcount) \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    776\u001b[0m                  (((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lexicon_coding\u001b[38;5;241m.\u001b[39mboundaries \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    777\u001b[0m                     addcount) \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    778\u001b[0m                    math\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lexicon_coding\u001b[38;5;241m.\u001b[39mboundaries\n\u001b[1;32m    779\u001b[0m                             \u001b[38;5;241m+\u001b[39m addcount))\n\u001b[1;32m    780\u001b[0m                   \u001b[38;5;241m-\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lexicon_coding\u001b[38;5;241m.\u001b[39mboundaries\n\u001b[1;32m    781\u001b[0m                      \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lexicon_coding\u001b[38;5;241m.\u001b[39mboundaries))\n\u001b[1;32m    782\u001b[0m                   \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lexicon_coding\u001b[38;5;241m.\u001b[39mget_codelength(\n\u001b[1;32m    783\u001b[0m                       construction))\n\u001b[1;32m    784\u001b[0m                  \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_corpus_coding\u001b[38;5;241m.\u001b[39mweight)\n\u001b[1;32m    785\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(construction) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    786\u001b[0m     cost \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m badlikelihood\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Caricare il dataset\n",
    "df_test = pd.read_csv('train.csv')\n",
    "\n",
    "# Estrarre i titoli\n",
    "titles = df_test['Description']\n",
    "\n",
    "# Caricare il modello Morfessor\n",
    "io = morfessor.MorfessorIO()\n",
    "model = io.read_binary_model_file(\"model.bin\")\n",
    "\n",
    "# Applicare il modello Morfessor a ogni titolo\n",
    "tokenized_titles = []\n",
    "for title in titles:\n",
    "    segmentation = model.viterbi_segment(title)[0]\n",
    "    tokenized_titles.append(segmentation)\n",
    "\n",
    "# Aggiungere i titoli tokenizzati al dataframe\n",
    "df_test['Tokenized_Title'] = tokenized_titles\n",
    "\n",
    "# Visualizzare i risultati\n",
    "print(df_test[['Title', 'Tokenized_Title']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0df59c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import morfessor\n",
    "\n",
    "# Funzione per tokenizzare un titolo utilizzando Morfessor\n",
    "def tokenize_title(title, model):\n",
    "    if pd.isna(title):\n",
    "        return \"\"\n",
    "    return ' '.join(model.viterbi_segment(title)[0])\n",
    "\n",
    "# Caricare il dataset\n",
    "df_test = pd.read_csv('train.csv')\n",
    "\n",
    "# Caricare il modello Morfessor\n",
    "io = morfessor.MorfessorIO()\n",
    "model = io.read_binary_model_file(\"model.bin\")\n",
    "\n",
    "# Applicare il modello Morfessor a ogni titolo utilizzando la comprensione di liste\n",
    "# Questo è più efficiente rispetto a un ciclo for\n",
    "df_test['Tokenized_Title'] = df_test['Description'].apply(lambda x: tokenize_title(x, model))\n",
    "\n",
    "# Visualizzare i risultati\n",
    "print(df_test[['Title', 'Tokenized_Title']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bcad092",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "...........................................................\n",
      "...........................................................\n",
      "...........................................................\n",
      "...........................................................\n",
      "...........................................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  Title  \\\n",
      "0                     Fears for T N pension after talks   \n",
      "1     The Race is On: Second Private Team Sets Launc...   \n",
      "2         Ky. Company Wins Grant to Study Peptides (AP)   \n",
      "3         Prediction Unit Helps Forecast Wildfires (AP)   \n",
      "4           Calif. Aims to Limit Farm-Related Smog (AP)   \n",
      "...                                                 ...   \n",
      "7595                                   Around the world   \n",
      "7596                        Void is filled with Clement   \n",
      "7597                             Martinez leaves bitter   \n",
      "7598  5 of arthritis patients in Singapore take Bext...   \n",
      "7599                             EBay gets into rentals   \n",
      "\n",
      "                                        Tokenized_Title  \n",
      "0             [Fear, s,  for T N pension after , talks]  \n",
      "1     [The,  Race is On: Second , Private,  Team Set...  \n",
      "2     [Ky.,  , Company,  Wins Grant to Study Peptide...  \n",
      "3     [Predict, ion,  Unit Helps , Forecast,  Wildfi...  \n",
      "4     [Calif.,  Aims to Limit , Farm, -Related,  Smo...  \n",
      "...                                                 ...  \n",
      "7595                             [Around,  the , world]  \n",
      "7596                  [Void,  is filled with , Clement]  \n",
      "7597                       [Martinez,  leaves , bitter]  \n",
      "7598  [5 of arthritis patients in , Singapore,  take...  \n",
      "7599                     [EBay,  gets into , rental, s]  \n",
      "\n",
      "[7600 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import morfessor\n",
    "import pandas as pd\n",
    "\n",
    "def process_dataset(train_path, test_path, title_column='Title'):\n",
    "    # Caricare il dataset di training\n",
    "    train_set = pd.read_csv(train_path)\n",
    "\n",
    "    # Estrazione dei titoli\n",
    "    titoli = train_set[title_column]\n",
    "\n",
    "    # Salvare i titoli in un nuovo file di testo\n",
    "    path_del_file_titoli = 'titoli_per_morfessor.txt'\n",
    "    with open(path_del_file_titoli, 'w', encoding='utf-8') as file:\n",
    "        for titolo in titoli:\n",
    "            file.write(titolo + '\\n')\n",
    "\n",
    "    # Funzione per aggiustare i conteggi di ogni composto\n",
    "    def log_func(x):\n",
    "        return int(round(math.log(x + 1, 2)))\n",
    "\n",
    "    # Creazione di un'istanza per I/O e del modello Morfessor\n",
    "    io = morfessor.MorfessorIO()\n",
    "    train_data = list(io.read_corpus_file(path_del_file_titoli))\n",
    "\n",
    "    # Creare un'istanza del modello Morfessor e caricare i dati\n",
    "    model = morfessor.BaselineModel()\n",
    "    model.load_data(train_data, count_modifier=log_func)\n",
    "\n",
    "    # Addestrare il modello\n",
    "    model.train_batch()\n",
    "\n",
    "    # Salvare il modello addestrato\n",
    "    model_path = \"model.bin\"\n",
    "    io.write_binary_model_file(model_path, model)\n",
    "\n",
    "    # Caricare il dataset di test\n",
    "    df_test = pd.read_csv(test_path)\n",
    "\n",
    "    # Estrarre i titoli\n",
    "    titles = df_test[title_column]\n",
    "\n",
    "    # Caricare il modello Morfessor\n",
    "    model = io.read_binary_model_file(model_path)\n",
    "\n",
    "    # Applicare il modello Morfessor a ogni titolo\n",
    "    tokenized_titles = []\n",
    "    for title in titles:\n",
    "        segmentation = model.viterbi_segment(title)[0]\n",
    "        tokenized_titles.append(segmentation)\n",
    "\n",
    "    # Aggiungere i titoli tokenizzati al dataframe\n",
    "    df_test['Tokenized_Title'] = tokenized_titles\n",
    "\n",
    "    # Visualizzare i risultati\n",
    "    return df_test[['Title', 'Tokenized_Title']]\n",
    "\n",
    "# Utilizzo della funzione\n",
    "train_path = 'train.csv'\n",
    "test_path = 'test.csv'\n",
    "result_df = process_dataset(train_path, test_path)\n",
    "print(result_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2956ceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Tokenized_Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fears for T N pension after talks</td>\n",
       "      <td>[Fear, s,  for T N pension after , talks]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Race is On: Second Private Team Sets Launc...</td>\n",
       "      <td>[The,  Race is On: Second , Private,  Team Set...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ky. Company Wins Grant to Study Peptides (AP)</td>\n",
       "      <td>[Ky.,  , Company,  Wins Grant to Study Peptide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Prediction Unit Helps Forecast Wildfires (AP)</td>\n",
       "      <td>[Predict, ion,  Unit Helps , Forecast,  Wildfi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Calif. Aims to Limit Farm-Related Smog (AP)</td>\n",
       "      <td>[Calif.,  Aims to Limit , Farm, -Related,  Smo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7595</th>\n",
       "      <td>Around the world</td>\n",
       "      <td>[Around,  the , world]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7596</th>\n",
       "      <td>Void is filled with Clement</td>\n",
       "      <td>[Void,  is filled with , Clement]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7597</th>\n",
       "      <td>Martinez leaves bitter</td>\n",
       "      <td>[Martinez,  leaves , bitter]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7598</th>\n",
       "      <td>5 of arthritis patients in Singapore take Bext...</td>\n",
       "      <td>[5 of arthritis patients in , Singapore,  take...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7599</th>\n",
       "      <td>EBay gets into rentals</td>\n",
       "      <td>[EBay,  gets into , rental, s]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7600 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Title  \\\n",
       "0                     Fears for T N pension after talks   \n",
       "1     The Race is On: Second Private Team Sets Launc...   \n",
       "2         Ky. Company Wins Grant to Study Peptides (AP)   \n",
       "3         Prediction Unit Helps Forecast Wildfires (AP)   \n",
       "4           Calif. Aims to Limit Farm-Related Smog (AP)   \n",
       "...                                                 ...   \n",
       "7595                                   Around the world   \n",
       "7596                        Void is filled with Clement   \n",
       "7597                             Martinez leaves bitter   \n",
       "7598  5 of arthritis patients in Singapore take Bext...   \n",
       "7599                             EBay gets into rentals   \n",
       "\n",
       "                                        Tokenized_Title  \n",
       "0             [Fear, s,  for T N pension after , talks]  \n",
       "1     [The,  Race is On: Second , Private,  Team Set...  \n",
       "2     [Ky.,  , Company,  Wins Grant to Study Peptide...  \n",
       "3     [Predict, ion,  Unit Helps , Forecast,  Wildfi...  \n",
       "4     [Calif.,  Aims to Limit , Farm, -Related,  Smo...  \n",
       "...                                                 ...  \n",
       "7595                             [Around,  the , world]  \n",
       "7596                  [Void,  is filled with , Clement]  \n",
       "7597                       [Martinez,  leaves , bitter]  \n",
       "7598  [5 of arthritis patients in , Singapore,  take...  \n",
       "7599                     [EBay,  gets into , rental, s]  \n",
       "\n",
       "[7600 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e081e11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "...........................................................\n",
      "...........................................................\n",
      "...........................................................\n",
      "...........................................................\n",
      "...........................................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  Title  \\\n",
      "0                     Fears for T N pension after talks   \n",
      "1     The Race is On: Second Private Team Sets Launc...   \n",
      "2         Ky. Company Wins Grant to Study Peptides (AP)   \n",
      "3         Prediction Unit Helps Forecast Wildfires (AP)   \n",
      "4           Calif. Aims to Limit Farm-Related Smog (AP)   \n",
      "...                                                 ...   \n",
      "7595                                   Around the world   \n",
      "7596                        Void is filled with Clement   \n",
      "7597                             Martinez leaves bitter   \n",
      "7598  5 of arthritis patients in Singapore take Bext...   \n",
      "7599                             EBay gets into rentals   \n",
      "\n",
      "                                        Tokenized_Title  \n",
      "0             [Fear, s,  for T N pension after , talks]  \n",
      "1     [The,  Race is On: Second , Private,  Team Set...  \n",
      "2     [Ky.,  , Company,  Wins Grant to Study Peptide...  \n",
      "3     [Predict, ion,  Unit Helps , Forecast,  Wildfi...  \n",
      "4     [Calif.,  Aims to Limit , Farm, -Related,  Smo...  \n",
      "...                                                 ...  \n",
      "7595                             [Around,  the , world]  \n",
      "7596                  [Void,  is filled with , Clement]  \n",
      "7597                       [Martinez,  leaves , bitter]  \n",
      "7598  [5 of arthritis patients in , Singapore,  take...  \n",
      "7599                     [EBay,  gets into , rental, s]  \n",
      "\n",
      "[7600 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import morfessor\n",
    "import pandas as pd\n",
    "\n",
    "class MorfessorTokenizer:\n",
    "    def __init__(self, title_column='Title'):\n",
    "        self.title_column = title_column\n",
    "        self.model = morfessor.BaselineModel()\n",
    "        self.path_del_file_titoli = 'titoli_per_morfessor.txt'\n",
    "        self.model_path = \"model.bin\"\n",
    "\n",
    "    def log_func(self, x):\n",
    "        return int(round(math.log(x + 1, 2)))\n",
    "\n",
    "    def train_model(self, train_path):\n",
    "        # Caricare il dataset di training\n",
    "        train_set = pd.read_csv(train_path)\n",
    "\n",
    "        # Estrazione dei titoli\n",
    "        titoli = train_set[self.title_column]\n",
    "\n",
    "        # Salvare i titoli in un nuovo file di testo\n",
    "        with open(self.path_del_file_titoli, 'w', encoding='utf-8') as file:\n",
    "            for titolo in titoli:\n",
    "                file.write(titolo + '\\n')\n",
    "\n",
    "        # Creazione di un'istanza per I/O\n",
    "        io = morfessor.MorfessorIO()\n",
    "        train_data = list(io.read_corpus_file(self.path_del_file_titoli))\n",
    "\n",
    "        # Caricare i dati nel modello\n",
    "        self.model.load_data(train_data, count_modifier=self.log_func)\n",
    "\n",
    "        # Addestrare il modello\n",
    "        self.model.train_batch()\n",
    "\n",
    "        # Salvare il modello addestrato\n",
    "        io.write_binary_model_file(self.model_path, self.model)\n",
    "\n",
    "    def tokenize_test_set(self, test_path):\n",
    "        # Caricare il dataset di test\n",
    "        df_test = pd.read_csv(test_path)\n",
    "\n",
    "        # Estrarre i titoli\n",
    "        titles = df_test[self.title_column]\n",
    "\n",
    "        # Caricare il modello Morfessor\n",
    "        io = morfessor.MorfessorIO()\n",
    "        model = io.read_binary_model_file(self.model_path)\n",
    "\n",
    "        # Applicare il modello Morfessor a ogni titolo\n",
    "        tokenized_titles = []\n",
    "        for title in titles:\n",
    "            segmentation = model.viterbi_segment(title)[0]\n",
    "            tokenized_titles.append(segmentation)\n",
    "\n",
    "        # Aggiungere i titoli tokenizzati al dataframe\n",
    "        df_test['Tokenized_Title'] = tokenized_titles\n",
    "\n",
    "        # Restituire il dataframe modificato\n",
    "        return df_test[['Title', 'Tokenized_Title']]\n",
    "\n",
    "# Utilizzo della pipeline\n",
    "tokenizer = MorfessorTokenizer()\n",
    "train_path = 'train.csv'\n",
    "test_path = 'test.csv'\n",
    "tokenizer.train_model(train_path)\n",
    "result_df = tokenizer.tokenize_test_set(test_path)\n",
    "print(result_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "286aba20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "...........................................................\n",
      "...........................................................\n",
      "...........................................................\n",
      "...........................................................\n",
      "...........................................................\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 67\u001b[0m\n\u001b[1;32m     63\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mtrain_model(train_path)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Supponiamo che tu abbia un DataFrame df_test da tokenizzare\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# df_test = pd.read_csv('test.csv')\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m result_df \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(df_test)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mprint\u001b[39m(result_df)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_test' is not defined"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import morfessor\n",
    "import pandas as pd\n",
    "\n",
    "class MorfessorTokenizer:\n",
    "    def __init__(self, title_column='Title'):\n",
    "        self.title_column = title_column\n",
    "        self.model = morfessor.BaselineModel()\n",
    "        self.path_del_file_titoli = 'titoli_per_morfessor.txt'\n",
    "        self.model_path = \"model.bin\"\n",
    "\n",
    "    def log_func(self, x):\n",
    "        return int(round(math.log(x + 1, 2)))\n",
    "\n",
    "    def train_model(self, train_path):\n",
    "        # Caricare il dataset di training\n",
    "        train_set = pd.read_csv(train_path)\n",
    "\n",
    "        # Estrazione dei titoli\n",
    "        titoli = train_set[self.title_column]\n",
    "\n",
    "        # Salvare i titoli in un nuovo file di testo\n",
    "        with open(self.path_del_file_titoli, 'w', encoding='utf-8') as file:\n",
    "            for titolo in titoli:\n",
    "                file.write(titolo + '\\n')\n",
    "\n",
    "        # Creazione di un'istanza per I/O\n",
    "        io = morfessor.MorfessorIO()\n",
    "        train_data = list(io.read_corpus_file(self.path_del_file_titoli))\n",
    "\n",
    "        # Caricare i dati nel modello\n",
    "        self.model.load_data(train_data, count_modifier=self.log_func)\n",
    "\n",
    "        # Addestrare il modello\n",
    "        self.model.train_batch()\n",
    "\n",
    "        # Salvare il modello addestrato\n",
    "        io.write_binary_model_file(self.model_path, self.model)\n",
    "\n",
    "    def tokenize(self, df):\n",
    "        # Caricare il modello Morfessor\n",
    "        io = morfessor.MorfessorIO()\n",
    "        model = io.read_binary_model_file(self.model_path)\n",
    "\n",
    "        # Estrarre i titoli\n",
    "        titles = df[self.title_column]\n",
    "\n",
    "        # Applicare il modello Morfessor a ogni titolo\n",
    "        tokenized_titles = []\n",
    "        for title in titles:\n",
    "            segmentation = model.viterbi_segment(title)[0]\n",
    "            tokenized_titles.append(segmentation)\n",
    "\n",
    "        # Aggiungere i titoli tokenizzati al dataframe\n",
    "        df['Tokenized_Title'] = tokenized_titles\n",
    "\n",
    "        # Restituire il dataframe modificato\n",
    "        return df\n",
    "\n",
    "# Utilizzo della pipeline\n",
    "tokenizer = MorfessorTokenizer()\n",
    "train_path = 'train.csv'\n",
    "tokenizer.train_model(train_path)\n",
    "\n",
    "# Supponiamo che tu abbia un DataFrame df_test da tokenizzare\n",
    "# df_test = pd.read_csv('test.csv')\n",
    "result_df = tokenizer.tokenize(df_test)\n",
    "print(result_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e6c22b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class Index</th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reuters)</td>\n",
       "      <td>Reuters - Short-sellers, Wall Street's dwindli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "      <td>Reuters - Private investment firm Carlyle Grou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters)</td>\n",
       "      <td>Reuters - Soaring crude prices plus worries\\ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "      <td>Reuters - Authorities have halted oil export\\f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "      <td>AFP - Tearaway world oil prices, toppling reco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119995</th>\n",
       "      <td>1</td>\n",
       "      <td>Pakistan's Musharraf Says Won't Quit as Army C...</td>\n",
       "      <td>KARACHI (Reuters) - Pakistani President Perve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119996</th>\n",
       "      <td>2</td>\n",
       "      <td>Renteria signing a top-shelf deal</td>\n",
       "      <td>Red Sox general manager Theo Epstein acknowled...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119997</th>\n",
       "      <td>2</td>\n",
       "      <td>Saban not going to Dolphins yet</td>\n",
       "      <td>The Miami Dolphins will put their courtship of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119998</th>\n",
       "      <td>2</td>\n",
       "      <td>Today's NFL games</td>\n",
       "      <td>PITTSBURGH at NY GIANTS Time: 1:30 p.m. Line: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119999</th>\n",
       "      <td>2</td>\n",
       "      <td>Nets get Carter from Raptors</td>\n",
       "      <td>INDIANAPOLIS -- All-Star Vince Carter was trad...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Class Index                                              Title  \\\n",
       "0                 3  Wall St. Bears Claw Back Into the Black (Reuters)   \n",
       "1                 3  Carlyle Looks Toward Commercial Aerospace (Reu...   \n",
       "2                 3    Oil and Economy Cloud Stocks' Outlook (Reuters)   \n",
       "3                 3  Iraq Halts Oil Exports from Main Southern Pipe...   \n",
       "4                 3  Oil prices soar to all-time record, posing new...   \n",
       "...             ...                                                ...   \n",
       "119995            1  Pakistan's Musharraf Says Won't Quit as Army C...   \n",
       "119996            2                  Renteria signing a top-shelf deal   \n",
       "119997            2                    Saban not going to Dolphins yet   \n",
       "119998            2                                  Today's NFL games   \n",
       "119999            2                       Nets get Carter from Raptors   \n",
       "\n",
       "                                              Description  \n",
       "0       Reuters - Short-sellers, Wall Street's dwindli...  \n",
       "1       Reuters - Private investment firm Carlyle Grou...  \n",
       "2       Reuters - Soaring crude prices plus worries\\ab...  \n",
       "3       Reuters - Authorities have halted oil export\\f...  \n",
       "4       AFP - Tearaway world oil prices, toppling reco...  \n",
       "...                                                   ...  \n",
       "119995   KARACHI (Reuters) - Pakistani President Perve...  \n",
       "119996  Red Sox general manager Theo Epstein acknowled...  \n",
       "119997  The Miami Dolphins will put their courtship of...  \n",
       "119998  PITTSBURGH at NY GIANTS Time: 1:30 p.m. Line: ...  \n",
       "119999  INDIANAPOLIS -- All-Star Vince Carter was trad...  \n",
       "\n",
       "[120000 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3935a208",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import morfessor\n",
    "import pandas as pd\n",
    "\n",
    "class MorfessorPipeline:\n",
    "    def __init__(self, infile, model_path=\"model.bin\"):\n",
    "        self.infile = infile\n",
    "        self.model_path = model_path\n",
    "        self.model = None\n",
    "\n",
    "    def log_func(self, x):\n",
    "        return int(round(math.log(x + 1, 2)))\n",
    "\n",
    "    def train_model(self):\n",
    "        # Creazione di un'istanza per I/O e del modello Morfessor\n",
    "        io = morfessor.MorfessorIO()\n",
    "        train_data = list(io.read_corpus_file(self.infile))\n",
    "\n",
    "        # Creare un'istanza del modello Morfessor e caricare i dati\n",
    "        self.model = morfessor.BaselineModel()\n",
    "        self.model.load_data(train_data, count_modifier=self.log_func)\n",
    "\n",
    "        # Addestrare il modello\n",
    "        self.model.train_batch()\n",
    "\n",
    "        # Salvare il modello addestrato\n",
    "        io.write_binary_model_file(self.model_path, self.model)\n",
    "\n",
    "    def tokenize_column(self, df, column_name):\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Modello non addestrato. Si prega di addestrare il modello prima di tokenizzare.\")\n",
    "\n",
    "        # Caricare il modello Morfessor\n",
    "        io = morfessor.MorfessorIO()\n",
    "        model = io.read_binary_model_file(self.model_path)\n",
    "\n",
    "        # Applicare il modello Morfessor alla colonna specificata\n",
    "        tokenized_data = []\n",
    "        for item in df[column_name]:\n",
    "            segmentation = model.viterbi_segment(item)[0]\n",
    "            tokenized_data.append(segmentation)\n",
    "\n",
    "        # Aggiungere i dati tokenizzati al dataframe\n",
    "        df['Tokenized'] = tokenized_data\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "73ceff88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "...........................................................\n",
      "...........................................................\n",
      "...........................................................\n",
      "...........................................................\n",
      "...........................................................\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m train_set \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Applicare la tokenizzazione\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m result_df \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mtokenize_column(train_set, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDescription\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(result_df\u001b[38;5;241m.\u001b[39mhead())\n",
      "Cell \u001b[0;32mIn[22], line 40\u001b[0m, in \u001b[0;36mMorfessorPipeline.tokenize_column\u001b[0;34m(self, df, column_name)\u001b[0m\n\u001b[1;32m     38\u001b[0m tokenized_data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m df[column_name]:\n\u001b[0;32m---> 40\u001b[0m     segmentation \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mviterbi_segment(item)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     41\u001b[0m     tokenized_data\u001b[38;5;241m.\u001b[39mappend(segmentation)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Aggiungere i dati tokenizzati al dataframe\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/morfessor/baseline.py:775\u001b[0m, in \u001b[0;36mBaselineModel.viterbi_segment\u001b[0;34m(self, compound, addcount, maxlen)\u001b[0m\n\u001b[1;32m    770\u001b[0m         cost \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (addcount \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39mlog(addcount) \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    771\u001b[0m                  \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lexicon_coding\u001b[38;5;241m.\u001b[39mget_codelength(\n\u001b[1;32m    772\u001b[0m                      construction)\n\u001b[1;32m    773\u001b[0m                  \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_corpus_coding\u001b[38;5;241m.\u001b[39mweight)\n\u001b[1;32m    774\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 775\u001b[0m         cost \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (logtokens \u001b[38;5;241m-\u001b[39m math\u001b[38;5;241m.\u001b[39mlog(addcount) \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    776\u001b[0m                  (((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lexicon_coding\u001b[38;5;241m.\u001b[39mboundaries \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    777\u001b[0m                     addcount) \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    778\u001b[0m                    math\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lexicon_coding\u001b[38;5;241m.\u001b[39mboundaries\n\u001b[1;32m    779\u001b[0m                             \u001b[38;5;241m+\u001b[39m addcount))\n\u001b[1;32m    780\u001b[0m                   \u001b[38;5;241m-\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lexicon_coding\u001b[38;5;241m.\u001b[39mboundaries\n\u001b[1;32m    781\u001b[0m                      \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lexicon_coding\u001b[38;5;241m.\u001b[39mboundaries))\n\u001b[1;32m    782\u001b[0m                   \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lexicon_coding\u001b[38;5;241m.\u001b[39mget_codelength(\n\u001b[1;32m    783\u001b[0m                       construction))\n\u001b[1;32m    784\u001b[0m                  \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_corpus_coding\u001b[38;5;241m.\u001b[39mweight)\n\u001b[1;32m    785\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(construction) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    786\u001b[0m     cost \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m badlikelihood\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Utilizzo della pipeline\n",
    "pipeline = MorfessorPipeline('titoli_per_morfessor.txt')\n",
    "pipeline.train_model()\n",
    "\n",
    "# Caricare il dataset\n",
    "train_set = pd.read_csv('train.csv')\n",
    "\n",
    "# Applicare la tokenizzazione\n",
    "result_df = pipeline.tokenize_column(train_set, 'Description')\n",
    "print(result_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1a343e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "...........................................................\n",
      "...........................................................\n",
      "..........................................................."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testo tokenizzato: ['Esempio di testo da tokenizz', 'are', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import morfessor\n",
    "import pandas as pd\n",
    "\n",
    "class MorfessorPipeline:\n",
    "    def __init__(self):\n",
    "        self.model = morfessor.BaselineModel()\n",
    "\n",
    "    def train(self, training_data_path):\n",
    "        io = morfessor.MorfessorIO()\n",
    "        data = list(io.read_corpus_file(training_data_path))\n",
    "        self.model.load_data(data)\n",
    "        self.model.train_batch()\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return self.model.viterbi_segment(text)[0]\n",
    "\n",
    "# Uso della Pipeline\n",
    "pipeline = MorfessorPipeline()\n",
    "\n",
    "# Percorso del file di testo per l'addestramento\n",
    "training_data_path = 'titoli_per_morfessor.txt'\n",
    "pipeline.train(training_data_path)\n",
    "\n",
    "# Tokenizzazione di un esempio di testo\n",
    "test_text = \"Esempio di testo da tokenizzare.\"\n",
    "tokenized_text = pipeline.tokenize(test_text)\n",
    "print(\"Testo tokenizzato:\", tokenized_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b6437b37",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 1 fields in line 5, saw 2\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m MorfessorPipeline()\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Caricare il DataFrame e addestrare il modello\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m df_train \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitoli_per_morfessor.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     26\u001b[0m pipeline\u001b[38;5;241m.\u001b[39mtrain(df_train, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myour_column_name\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Tokenizzazione di un esempio di testo\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:617\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mread(nrows)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1748\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1741\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m     (\n\u001b[1;32m   1745\u001b[0m         index,\n\u001b[1;32m   1746\u001b[0m         columns,\n\u001b[1;32m   1747\u001b[0m         col_dict,\n\u001b[0;32m-> 1748\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mread(  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m   1749\u001b[0m         nrows\n\u001b[1;32m   1750\u001b[0m     )\n\u001b[1;32m   1751\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1752\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mread_low_memory(nrows)\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32mparsers.pyx:843\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:904\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:879\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:890\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2058\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 1 fields in line 5, saw 2\n"
     ]
    }
   ],
   "source": [
    "import morfessor\n",
    "import pandas as pd\n",
    "\n",
    "class MorfessorPipeline:\n",
    "    def __init__(self):\n",
    "        self.model = morfessor.BaselineModel()\n",
    "\n",
    "    def train(self, df, column):\n",
    "        # Estrai il testo da una colonna del DataFrame\n",
    "        texts = df[column].dropna().tolist()\n",
    "\n",
    "        # Carica i dati e addestra il modello\n",
    "        data = [(text, 1) for text in texts]  # Assumiamo frequenza 1 per semplicità\n",
    "        self.model.load_data(data)\n",
    "        self.model.train_batch()\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        # Tokenizza il testo utilizzando il modello Morfessor\n",
    "        return self.model.viterbi_segment(text)[0]\n",
    "\n",
    "# Esempio di utilizzo della pipeline\n",
    "pipeline = MorfessorPipeline()\n",
    "\n",
    "# Caricare il DataFrame e addestrare il modello\n",
    "df_train = pd.read_csv('titoli_per_morfessor.txt')\n",
    "pipeline.train(df_train, 'your_column_name')\n",
    "\n",
    "# Tokenizzazione di un esempio di testo\n",
    "test_text = \"Esempio di testo da tokenizzare.\"\n",
    "tokenized_text = pipeline.tokenize(test_text)\n",
    "print(\"Testo tokenizzato:\", tokenized_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aef6a71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
